{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "logits class: tensor([[ 0.0591,  0.0649, -0.0289, -0.0455,  0.0225,  0.0255,  0.0090,  0.0439,\n",
      "          0.0767, -0.0674]], grad_fn=<AddmmBackward0>)\n",
      "Predicted class: tensor([8])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "print(f\"logits class: {logits}\")\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "\n",
    "# print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size:  torch.Size([3, 28, 28])\n",
      "flatten_size:  torch.Size([3, 784])\n",
      "hidden1_size:  torch.Size([3, 20])\n",
      "logits_size:  torch.Size([3, 10])\n",
      "tensor([[0.1133, 0.1009, 0.0904, 0.0938, 0.0776, 0.1022, 0.1153, 0.0987, 0.0890,\n",
      "         0.1188],\n",
      "        [0.0908, 0.0826, 0.1086, 0.0998, 0.0863, 0.0887, 0.1172, 0.0961, 0.1055,\n",
      "         0.1244],\n",
      "        [0.0988, 0.1006, 0.0962, 0.0967, 0.0766, 0.1020, 0.1190, 0.0916, 0.1055,\n",
      "         0.1128]], grad_fn=<SoftmaxBackward0>)\n",
      "pred_probab_size:  torch.Size([3, 10])\n",
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : Parameter containing:\n",
      "tensor([[ 0.0032, -0.0028,  0.0018,  ...,  0.0074, -0.0136,  0.0348],\n",
      "        [ 0.0185, -0.0142, -0.0316,  ...,  0.0337,  0.0155,  0.0172],\n",
      "        [ 0.0118, -0.0043, -0.0123,  ..., -0.0183, -0.0288,  0.0133],\n",
      "        ...,\n",
      "        [ 0.0056, -0.0075, -0.0321,  ...,  0.0239, -0.0038, -0.0034],\n",
      "        [-0.0229,  0.0283, -0.0268,  ...,  0.0186,  0.0257, -0.0144],\n",
      "        [ 0.0062,  0.0171,  0.0080,  ..., -0.0220,  0.0014, -0.0073]],\n",
      "       requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([ 3.0492e-02, -3.1207e-02,  6.1574e-03, -2.3029e-02,  1.1717e-02,\n",
      "        -2.9337e-02,  3.3456e-02,  6.8059e-04, -4.4261e-03, -2.5702e-03,\n",
      "         3.3977e-02, -3.0816e-02,  3.5187e-03,  1.4829e-02, -1.3872e-04,\n",
      "         1.9699e-03, -2.9589e-02, -2.5814e-03, -9.3482e-03,  1.0062e-02,\n",
      "         4.1806e-03, -5.4886e-03, -1.3477e-02,  8.9643e-03, -3.5358e-02,\n",
      "        -1.9033e-02,  1.0518e-02,  1.4044e-02,  1.8929e-02,  2.9853e-02,\n",
      "         1.7106e-02,  4.7828e-03,  2.4427e-02, -3.1017e-02, -7.4422e-03,\n",
      "         1.2692e-02, -3.2996e-02, -1.6522e-02, -1.3212e-02,  1.0095e-02,\n",
      "        -4.1435e-03, -2.8068e-02, -3.4333e-02,  2.6187e-02, -1.7285e-02,\n",
      "        -2.4840e-02, -3.6558e-03, -1.2919e-02, -1.8344e-02, -1.6394e-02,\n",
      "        -1.8973e-02,  4.5859e-03, -3.1507e-03, -1.4641e-02,  2.4725e-02,\n",
      "         3.2045e-02,  1.9159e-02, -1.9302e-03, -2.2649e-02, -4.6765e-03,\n",
      "        -3.1385e-02, -2.5464e-02,  5.7250e-05, -2.4578e-02, -2.2174e-02,\n",
      "         1.1949e-02, -2.0132e-02,  3.2747e-02,  3.4068e-02, -3.3738e-02,\n",
      "         3.2757e-02,  2.9339e-02,  2.1849e-02,  2.8054e-02,  8.2211e-03,\n",
      "         2.2886e-02, -5.6173e-04, -3.3758e-02,  3.9013e-03, -2.8949e-02,\n",
      "        -4.2220e-03, -2.4511e-02,  3.5109e-02,  2.7970e-02,  1.4851e-03,\n",
      "        -3.4844e-02, -2.1879e-02,  1.0085e-02,  5.7986e-03, -1.7082e-02,\n",
      "        -3.3472e-02,  2.7374e-02,  8.7993e-03,  1.2886e-02,  9.5926e-03,\n",
      "        -1.0106e-02,  3.7436e-03, -5.5216e-03,  3.4792e-02,  2.5100e-02,\n",
      "         1.1940e-02, -2.8051e-02,  3.2534e-02, -5.0260e-04,  1.4525e-02,\n",
      "         1.9833e-02,  1.3900e-02, -1.3938e-02,  3.6104e-03, -2.1103e-03,\n",
      "         1.1109e-02,  2.1501e-02,  8.2409e-04,  3.1799e-02,  2.7957e-02,\n",
      "         1.7723e-02,  7.0201e-03,  6.1745e-03,  1.9778e-02,  1.7071e-02,\n",
      "         1.6558e-02,  1.9764e-02,  6.6279e-03,  1.3146e-02,  3.2769e-02,\n",
      "        -2.8205e-02, -1.2174e-03,  3.0436e-02,  2.3816e-02,  2.5117e-02,\n",
      "         5.9734e-03, -2.5559e-02,  1.6179e-02, -1.8350e-02,  3.0443e-02,\n",
      "         2.7341e-02, -2.8679e-02, -3.3517e-02, -1.7949e-02, -2.0216e-02,\n",
      "        -3.0052e-02, -9.0494e-03, -1.0957e-02, -1.0466e-02, -8.1368e-03,\n",
      "        -1.5418e-02,  2.8784e-02,  2.4566e-02, -2.0685e-02,  1.9342e-02,\n",
      "         9.0727e-03, -3.3477e-02, -1.1222e-02, -2.9082e-02, -2.3737e-02,\n",
      "        -1.2566e-02, -2.6155e-02, -9.1389e-03, -5.5378e-03, -8.9398e-03,\n",
      "        -1.4638e-02, -7.1346e-03, -6.9621e-03, -1.1616e-02, -3.1985e-02,\n",
      "        -6.1173e-03,  2.1533e-02, -2.7359e-02,  2.0121e-02,  2.0307e-02,\n",
      "         2.5568e-02,  2.1432e-02,  1.1766e-02, -2.9236e-02,  2.9700e-02,\n",
      "        -1.2347e-02,  3.1788e-02,  1.0959e-02, -2.9458e-02,  2.9226e-02,\n",
      "        -3.4377e-02,  4.5529e-03, -1.2003e-02,  2.6536e-02, -1.4161e-02,\n",
      "        -3.1099e-02, -1.6497e-02,  7.1889e-03,  1.2685e-02, -2.6566e-02,\n",
      "         1.9831e-02, -1.9536e-02, -1.7703e-03,  2.5426e-02,  1.0014e-02,\n",
      "         1.2029e-02,  9.5893e-04,  7.5175e-03, -1.5912e-02, -4.8417e-03,\n",
      "        -1.0168e-02, -1.7062e-02, -8.3384e-03,  1.5732e-02, -1.7316e-02,\n",
      "        -2.0439e-02, -1.1865e-03, -3.4489e-02, -1.4056e-02,  1.7990e-02,\n",
      "        -4.6021e-03, -2.4176e-02, -3.2672e-02,  9.3896e-03, -1.0526e-02,\n",
      "         8.9714e-03,  1.4014e-02,  2.2099e-02,  2.9757e-02, -3.5143e-02,\n",
      "        -3.2487e-02,  3.4133e-02, -7.3026e-03,  1.4498e-02, -1.8178e-02,\n",
      "        -2.1893e-02,  6.7587e-03, -1.0050e-02,  7.4970e-03, -1.4634e-02,\n",
      "         8.0956e-04,  1.0641e-02, -7.2955e-03,  1.1704e-02, -3.4146e-02,\n",
      "         2.2931e-02, -2.6373e-02, -3.1169e-02,  1.9106e-02,  5.5564e-03,\n",
      "         4.9506e-03, -3.1916e-02,  1.7879e-02,  2.9808e-02, -3.3908e-02,\n",
      "         8.8876e-03,  1.4088e-02, -4.0260e-03,  1.3614e-02, -1.0795e-02,\n",
      "        -9.7616e-03,  2.3515e-03,  2.6040e-02,  3.1066e-02, -1.7185e-02,\n",
      "        -2.6673e-02,  5.2291e-03,  1.1550e-02, -1.0534e-02, -7.4977e-03,\n",
      "         1.1211e-02,  2.7149e-02, -1.1911e-02, -1.2301e-03, -3.3709e-02,\n",
      "         1.3048e-02, -3.7805e-03, -1.5334e-02,  5.5752e-03,  2.6869e-02,\n",
      "         1.5758e-02,  1.2409e-02, -1.2524e-02,  3.0658e-02, -2.0789e-02,\n",
      "        -3.2533e-02, -3.1266e-02,  3.1319e-02,  1.7341e-02,  3.4463e-02,\n",
      "         1.3925e-02, -1.7034e-02, -1.9660e-02, -1.0390e-02,  1.1450e-02,\n",
      "        -2.0550e-02,  1.1159e-03, -5.9531e-03, -2.1799e-03, -3.3688e-02,\n",
      "         1.4703e-02,  4.8980e-03, -3.3905e-02,  2.1586e-02, -1.4676e-02,\n",
      "         2.5686e-02,  3.0701e-02,  3.0263e-02, -6.6185e-03,  1.2036e-02,\n",
      "         1.2545e-02, -2.3782e-02,  2.8816e-04, -5.9997e-03,  2.0634e-02,\n",
      "         1.4204e-02,  2.9580e-02,  2.8931e-02, -3.2977e-02, -3.3201e-02,\n",
      "        -8.9750e-03,  1.9690e-02, -1.4919e-02, -8.7800e-03,  1.9936e-02,\n",
      "         1.6815e-02, -3.3626e-02,  3.5435e-02, -1.0734e-02,  2.5929e-02,\n",
      "        -4.8875e-03,  2.4072e-02,  1.0574e-02,  3.3962e-02,  7.7428e-03,\n",
      "         1.2174e-03, -2.1719e-02,  2.2767e-02, -2.3619e-02, -3.4577e-02,\n",
      "         3.5092e-02,  3.4479e-02, -2.4808e-02,  1.1952e-02,  1.8649e-03,\n",
      "        -7.7805e-03, -1.7263e-02,  1.0296e-02, -1.1891e-03,  2.3631e-02,\n",
      "         2.1043e-02,  2.1841e-02,  1.9799e-02,  2.9532e-02, -3.5086e-02,\n",
      "        -2.8164e-02, -1.6642e-03,  1.4785e-02, -1.7444e-02, -2.1408e-02,\n",
      "        -2.2492e-02, -2.9921e-02, -3.4544e-02,  3.0965e-02, -3.3151e-02,\n",
      "         6.5670e-03,  2.4121e-02, -1.0641e-02,  3.5569e-02,  3.1864e-02,\n",
      "        -2.3681e-02, -2.8276e-02,  8.1191e-03,  6.4206e-03, -2.2387e-02,\n",
      "        -2.1370e-02, -9.8062e-03, -1.2524e-02, -2.5059e-02, -2.5224e-02,\n",
      "        -2.4802e-02, -2.4642e-02,  2.3529e-03, -2.7164e-02, -2.0896e-02,\n",
      "         3.0258e-02, -3.5589e-02, -1.9971e-02, -3.0053e-02,  1.3529e-02,\n",
      "         1.5722e-02,  3.7973e-03,  2.5644e-02,  9.2229e-04, -2.6038e-02,\n",
      "         5.3449e-03, -3.8956e-03, -2.5037e-02,  2.8596e-02,  5.0395e-04,\n",
      "         9.5827e-03, -7.4623e-03, -2.3274e-02,  1.6706e-02,  1.9664e-02,\n",
      "        -2.8946e-02,  8.3160e-03, -4.3800e-03,  7.2495e-03, -1.8596e-02,\n",
      "        -1.6674e-02,  7.8967e-03, -2.7180e-02,  4.8295e-03,  1.6784e-02,\n",
      "         3.0029e-02, -1.3774e-02,  3.4638e-02, -2.0915e-02, -2.2398e-02,\n",
      "         7.8474e-03, -2.9943e-02,  3.9718e-03, -2.1239e-02,  1.7280e-02,\n",
      "         2.8741e-02,  2.9205e-02,  2.0534e-02, -3.4735e-02,  1.3313e-02,\n",
      "        -4.4340e-03,  2.3480e-02,  3.3481e-02,  5.9816e-03, -3.2273e-02,\n",
      "         2.7766e-02, -1.1985e-02,  1.0256e-02,  7.9402e-03,  3.1607e-02,\n",
      "         2.4812e-02, -2.5899e-02,  1.6208e-02, -2.0048e-02,  4.6333e-03,\n",
      "         9.0969e-03,  1.2792e-02,  4.2315e-03, -3.2019e-02,  2.7023e-02,\n",
      "         8.1175e-03, -1.7392e-02, -1.5619e-02,  2.4143e-02,  1.9115e-02,\n",
      "        -1.3913e-02, -1.5976e-02, -1.0071e-02, -1.0137e-02,  1.3566e-02,\n",
      "         3.3621e-02, -1.4197e-02, -2.4301e-02, -8.6869e-03, -1.9153e-02,\n",
      "        -2.8955e-02, -3.4188e-02,  1.5930e-02,  1.0914e-02, -1.4267e-02,\n",
      "         3.4690e-02, -7.1509e-03, -3.1877e-02, -2.8866e-02, -3.5531e-02,\n",
      "        -2.3955e-03, -7.5206e-03,  3.4206e-02,  2.2395e-02,  3.2179e-02,\n",
      "        -2.4744e-02, -1.7861e-02, -9.1001e-03, -1.1694e-02, -4.5883e-03,\n",
      "        -3.2497e-03,  8.7473e-03, -1.4057e-02,  1.0700e-02, -1.6024e-02,\n",
      "         3.2444e-02, -2.9528e-02, -1.6126e-02,  1.0968e-02,  9.5482e-03,\n",
      "        -1.3355e-02, -8.5045e-03,  1.7031e-02, -5.0708e-03, -1.7856e-02,\n",
      "        -1.7687e-02,  2.6128e-02, -1.0450e-02,  2.1081e-02,  2.8854e-02,\n",
      "         3.2625e-02,  1.1426e-02, -1.8646e-02,  3.2499e-03, -2.7525e-02,\n",
      "        -2.4812e-02,  1.4274e-02,  9.0925e-03,  6.2850e-03, -2.3786e-02,\n",
      "         1.3263e-02,  2.8168e-02,  7.8423e-03, -7.5550e-03, -1.2085e-02,\n",
      "        -2.0466e-02,  1.3129e-02], requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : Parameter containing:\n",
      "tensor([[-0.0435, -0.0403, -0.0008,  ..., -0.0183,  0.0021,  0.0319],\n",
      "        [ 0.0389, -0.0337, -0.0404,  ...,  0.0050, -0.0119,  0.0022],\n",
      "        [-0.0434,  0.0202, -0.0344,  ..., -0.0033,  0.0430, -0.0324],\n",
      "        ...,\n",
      "        [-0.0322, -0.0105,  0.0327,  ..., -0.0382,  0.0078,  0.0211],\n",
      "        [-0.0240,  0.0161, -0.0164,  ..., -0.0258,  0.0144,  0.0234],\n",
      "        [-0.0264,  0.0004,  0.0383,  ...,  0.0061,  0.0340, -0.0221]],\n",
      "       requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
      "tensor([ 0.0275,  0.0373, -0.0377, -0.0101, -0.0303, -0.0196,  0.0107,  0.0046,\n",
      "         0.0231,  0.0101,  0.0396, -0.0298,  0.0353,  0.0159, -0.0120, -0.0400,\n",
      "         0.0341,  0.0066, -0.0341, -0.0282, -0.0401,  0.0224, -0.0044,  0.0204,\n",
      "         0.0385, -0.0013,  0.0419, -0.0254,  0.0357, -0.0290,  0.0216,  0.0182,\n",
      "         0.0124,  0.0276, -0.0441,  0.0138, -0.0055, -0.0245,  0.0412, -0.0439,\n",
      "        -0.0181, -0.0127, -0.0198,  0.0131,  0.0019,  0.0086,  0.0076, -0.0253,\n",
      "        -0.0077,  0.0405, -0.0069,  0.0106, -0.0373,  0.0266, -0.0262,  0.0078,\n",
      "         0.0077, -0.0268, -0.0029,  0.0317,  0.0075, -0.0059, -0.0041, -0.0260,\n",
      "        -0.0404, -0.0251, -0.0407, -0.0069,  0.0018,  0.0060,  0.0200,  0.0386,\n",
      "        -0.0028,  0.0290, -0.0012, -0.0011,  0.0242, -0.0129, -0.0415, -0.0243,\n",
      "         0.0135,  0.0268, -0.0072,  0.0178, -0.0348, -0.0048,  0.0372, -0.0185,\n",
      "         0.0195, -0.0050, -0.0399, -0.0183,  0.0340,  0.0063,  0.0284, -0.0386,\n",
      "        -0.0385,  0.0011,  0.0364,  0.0330, -0.0338,  0.0239,  0.0076, -0.0212,\n",
      "         0.0031, -0.0092, -0.0344,  0.0212,  0.0188,  0.0232, -0.0100,  0.0118,\n",
      "         0.0093, -0.0340,  0.0282, -0.0144, -0.0133, -0.0030,  0.0284,  0.0024,\n",
      "        -0.0100,  0.0300, -0.0242,  0.0048, -0.0291,  0.0090,  0.0320, -0.0382,\n",
      "        -0.0211,  0.0272,  0.0131, -0.0342, -0.0023,  0.0236, -0.0400,  0.0379,\n",
      "         0.0152, -0.0395,  0.0241,  0.0363,  0.0252, -0.0241,  0.0167, -0.0053,\n",
      "         0.0416, -0.0145,  0.0228, -0.0349, -0.0021,  0.0002, -0.0205,  0.0208,\n",
      "         0.0330, -0.0136,  0.0399,  0.0208,  0.0185,  0.0140,  0.0186,  0.0170,\n",
      "        -0.0320,  0.0382, -0.0369, -0.0242,  0.0364, -0.0343,  0.0407,  0.0188,\n",
      "        -0.0062,  0.0383,  0.0146, -0.0196, -0.0197, -0.0069,  0.0193,  0.0152,\n",
      "         0.0271, -0.0266,  0.0324,  0.0416, -0.0031,  0.0382, -0.0219,  0.0107,\n",
      "         0.0113,  0.0270, -0.0035, -0.0258,  0.0007,  0.0208, -0.0395,  0.0229,\n",
      "         0.0416,  0.0195, -0.0225, -0.0230,  0.0110,  0.0202, -0.0441, -0.0368,\n",
      "        -0.0301, -0.0053, -0.0021, -0.0428,  0.0375,  0.0415,  0.0212,  0.0305,\n",
      "        -0.0332, -0.0273,  0.0055, -0.0003,  0.0353, -0.0066, -0.0331, -0.0078,\n",
      "        -0.0277, -0.0395, -0.0011, -0.0019,  0.0228, -0.0155,  0.0146,  0.0132,\n",
      "        -0.0294,  0.0382,  0.0412, -0.0086, -0.0054, -0.0253, -0.0383,  0.0292,\n",
      "         0.0314,  0.0223,  0.0066, -0.0265, -0.0388,  0.0201,  0.0224, -0.0224,\n",
      "        -0.0162,  0.0022, -0.0060, -0.0151, -0.0400, -0.0206,  0.0099, -0.0234,\n",
      "         0.0345,  0.0395,  0.0096, -0.0034,  0.0438,  0.0415,  0.0057, -0.0254,\n",
      "         0.0441,  0.0102, -0.0227,  0.0403, -0.0264, -0.0207,  0.0220,  0.0013,\n",
      "        -0.0431, -0.0223, -0.0114, -0.0165,  0.0212,  0.0010, -0.0249, -0.0413,\n",
      "        -0.0029, -0.0312, -0.0285, -0.0325, -0.0068, -0.0314,  0.0106, -0.0390,\n",
      "        -0.0410,  0.0138, -0.0149,  0.0169,  0.0188, -0.0317,  0.0119, -0.0322,\n",
      "         0.0119, -0.0020,  0.0422,  0.0330,  0.0354, -0.0331, -0.0295,  0.0072,\n",
      "         0.0266, -0.0350, -0.0188,  0.0148, -0.0416, -0.0261,  0.0210,  0.0115,\n",
      "         0.0025,  0.0409, -0.0207,  0.0208,  0.0014,  0.0266,  0.0232,  0.0387,\n",
      "         0.0346,  0.0325, -0.0433, -0.0302, -0.0080,  0.0342,  0.0185,  0.0202,\n",
      "        -0.0126, -0.0304,  0.0031,  0.0368,  0.0439, -0.0384, -0.0344, -0.0312,\n",
      "        -0.0159,  0.0349, -0.0352,  0.0141, -0.0064, -0.0383,  0.0028,  0.0068,\n",
      "         0.0075,  0.0061, -0.0351, -0.0083,  0.0337,  0.0037, -0.0395, -0.0394,\n",
      "        -0.0267,  0.0230,  0.0297, -0.0252,  0.0209, -0.0106, -0.0229,  0.0339,\n",
      "        -0.0270, -0.0048, -0.0308,  0.0128, -0.0339, -0.0118, -0.0266, -0.0027,\n",
      "         0.0429, -0.0400,  0.0374, -0.0425,  0.0083, -0.0203, -0.0413, -0.0124,\n",
      "        -0.0345,  0.0264,  0.0422,  0.0094, -0.0141, -0.0387, -0.0070,  0.0334,\n",
      "         0.0278,  0.0255,  0.0385, -0.0058, -0.0238, -0.0256,  0.0223, -0.0229,\n",
      "        -0.0048, -0.0011,  0.0220, -0.0292,  0.0039,  0.0308,  0.0061,  0.0378,\n",
      "        -0.0113,  0.0160, -0.0433,  0.0413, -0.0112, -0.0415,  0.0177, -0.0241,\n",
      "         0.0260, -0.0236,  0.0215, -0.0118, -0.0113, -0.0012,  0.0112, -0.0222,\n",
      "        -0.0219, -0.0412, -0.0134, -0.0079,  0.0091,  0.0133, -0.0117,  0.0076,\n",
      "         0.0197,  0.0178,  0.0145,  0.0190,  0.0397, -0.0358,  0.0289,  0.0363,\n",
      "        -0.0189, -0.0399, -0.0264, -0.0044,  0.0403,  0.0118,  0.0218,  0.0239,\n",
      "         0.0427,  0.0423, -0.0307, -0.0385,  0.0262,  0.0371,  0.0056,  0.0427,\n",
      "        -0.0181, -0.0058, -0.0157,  0.0184,  0.0295,  0.0099, -0.0152, -0.0113,\n",
      "         0.0110, -0.0386, -0.0428,  0.0292,  0.0265, -0.0417,  0.0189, -0.0364,\n",
      "         0.0334, -0.0207,  0.0097,  0.0253,  0.0392, -0.0275, -0.0359,  0.0324,\n",
      "         0.0426, -0.0274, -0.0148, -0.0271, -0.0226, -0.0085, -0.0315,  0.0179,\n",
      "        -0.0047,  0.0052,  0.0110,  0.0316, -0.0241,  0.0380,  0.0347, -0.0171,\n",
      "         0.0439, -0.0125, -0.0411,  0.0259, -0.0439, -0.0283, -0.0072, -0.0185,\n",
      "         0.0275, -0.0056, -0.0051,  0.0191, -0.0214, -0.0263,  0.0409,  0.0434,\n",
      "         0.0225,  0.0054, -0.0117, -0.0361,  0.0292, -0.0276, -0.0185, -0.0193,\n",
      "        -0.0420,  0.0297, -0.0394, -0.0386,  0.0136, -0.0019, -0.0154,  0.0138],\n",
      "       requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : Parameter containing:\n",
      "tensor([[-0.0383,  0.0136, -0.0423,  ..., -0.0286,  0.0313,  0.0068],\n",
      "        [-0.0031,  0.0347, -0.0119,  ..., -0.0260, -0.0172, -0.0113],\n",
      "        [ 0.0440,  0.0022, -0.0022,  ...,  0.0143, -0.0314, -0.0250],\n",
      "        ...,\n",
      "        [ 0.0418, -0.0255, -0.0099,  ..., -0.0122,  0.0290,  0.0292],\n",
      "        [ 0.0067, -0.0390, -0.0364,  ...,  0.0085,  0.0325, -0.0362],\n",
      "        [ 0.0126, -0.0042, -0.0195,  ..., -0.0404,  0.0292, -0.0029]],\n",
      "       requires_grad=True) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : Parameter containing:\n",
      "tensor([-0.0067, -0.0004, -0.0288,  0.0302, -0.0295,  0.0360, -0.0158,  0.0171,\n",
      "        -0.0193,  0.0031], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print('input_size: ', input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print('flatten_size: ', flat_image.size())\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print('hidden1_size: ', hidden1.size())\n",
    "\n",
    "hidden1 = nn.ReLU()(hidden1) # 激活层\n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "print('logits_size: ', logits.size())\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)\n",
    "print('pred_probab_size: ', pred_probab.size())\n",
    "\n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}